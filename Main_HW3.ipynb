{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"urls.txt\",\"w\")\n",
    "\n",
    "for k in range(1,401):\n",
    "    \n",
    "    page = requests.get(\"https://www.atlasobscura.com/places?page=\"+str(k)+'&sort=likes_count') # select the page\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\") # take the content from the page\n",
    "\n",
    "\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    while(len(links)>30):\n",
    "\n",
    "        #remove None values\n",
    "        while(None in links):\n",
    "            links.remove(None)\n",
    "\n",
    "\n",
    "        #clean list\n",
    "        while('/places/new' in links):\n",
    "            links.remove('/places/new')\n",
    "\n",
    "\n",
    "        #remove urls from places in different categories\n",
    "        for link in links:\n",
    "\n",
    "    \n",
    "            if (link[:8]!='/places/'):\n",
    "                links.remove(link)\n",
    "\n",
    "    if (\"/places/\" not in links[-1]): links.pop()\n",
    "\n",
    "\n",
    "    links = links[9:] #remove urls from places in different categories\n",
    "\n",
    "    for i in range(len(links)):\n",
    "\n",
    "        links[i] = 'https://www.atlasobscura.com' + links[i] +'\\n'\n",
    "\n",
    "        f.write(links[i])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('urls.txt') as fh:\n",
    "    all_links = fh.read().split('\\n')\n",
    "\n",
    "cont_pages = 0\n",
    "name_pages = 0\n",
    "cont_folder = 1 \n",
    "\n",
    "#create directory if not already present\n",
    "if not os.path.exists('folder{}'.format(str(cont_folder))):\n",
    "    os.mkdir('folder{}'.format(str(cont_folder)))\n",
    "    \n",
    "    \n",
    "for line in all_links:\n",
    "    if cont_pages > 18:\n",
    "        cont_folder += 1\n",
    "        os.mkdir('folder{}'.format(str(cont_folder)))\n",
    "        cont_pages = 0\n",
    "\n",
    "    print(\"LINE: \" +line)\n",
    "    HTML = requests.get(line, allow_redirects=False, timeout=3) #request html\n",
    "    soup = bs4.BeautifulSoup(HTML.content, features=\"lxml\")\n",
    "    \n",
    "    #write html\n",
    "    with open(str('folder{}'.format(str(cont_folder)))+'/place'+str(name_pages)+'.txt', 'w') as f:\n",
    "        f.write(str(soup.prettify()))\n",
    "        f.close()\n",
    "        cont_pages += 1\n",
    "        name_pages += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_info is a function that when inputted a single html file it will clean and extract\n",
    "the necessary informations, appending them to the corresponding lists, so that we can unite\n",
    "everything in a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when it blocks remember to eleminate the elements if you restart from a certain point\n",
    "placeName = []\n",
    "placeTags = []\n",
    "numPeopleVisited = []\n",
    "numPeopleWant = []\n",
    "placeDesc = []\n",
    "placeShortDesc = []\n",
    "placeNearby = []\n",
    "placeAddress = []\n",
    "placeAlt = []\n",
    "placeLong = []\n",
    "placeEditors = []\n",
    "placePubDate = []\n",
    "placeRelatedLists = []\n",
    "placeRelatedPlaces = []\n",
    "placeURL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(html):\n",
    "    soup = bs4.BeautifulSoup(html)\n",
    "    \n",
    "    try:\n",
    "        placeName.append(soup.find_all('h1', {'class' : 'DDPage__header-title'})[0].contents[0])\n",
    "    except:\n",
    "        placeName.append('')\n",
    "    #place tags        \n",
    "    placeTags1 = [x.text for x in soup.find_all('span',{'class':'itemTags__tag js-item-tags-tag itemTags__tag--light itemTags__tag--rounded'})]\n",
    "    try:            \n",
    "        placeTags.append([x.replace('\\n','') for x in placeTags1])\n",
    "    except:\n",
    "        placeTags.append('')\n",
    "    \n",
    "    #N of people who visited\n",
    "    try:\n",
    "        numPeopleVisited.append(int(soup.find_all('div', {'class' : \"title-md item-action-count\"})[0].contents[0]))\n",
    "    except:\n",
    "        numPeopleVisited.append('')\n",
    "    \n",
    "    #number of people who want to go\n",
    "    try:\n",
    "        numPeopleWant.append(int(soup.find_all('div', {'class' : \"title-md item-action-count\"})[1].contents[0]))\n",
    "    except:\n",
    "        numPeopleWant.append('')\n",
    "        \n",
    "    #full description\n",
    "    try:\n",
    "        long_desc = [x.text for x in soup.find_all('div',{'id':'place-body'})]\n",
    "        clean_desc = [x.replace('\\n','') for x in long_desc]\n",
    "        placeDesc.append(clean_desc)\n",
    "    except:\n",
    "        placeDesc.append('')\n",
    "    \n",
    "    #short description of the place\n",
    "    try:\n",
    "        placeShortDesc.append(soup.find_all('h3', {'class' : 'DDPage__header-dek'})[0].contents[0])\n",
    "    except:\n",
    "        placeShortDesc.append('')\n",
    "        \n",
    "    #the nearby places\n",
    "    try:\n",
    "        placeNearby.append([x.text for x in soup.find_all('div', {'class' : 'DDPageSiderailRecirc__item-title'})])\n",
    "    except:\n",
    "        placeNearby.append('')\n",
    "        \n",
    "    #address of the place\n",
    "    try:\n",
    "        placeA =[x.text for x in soup.find_all('address', {'class' : 'DDPageSiderail__address'})]\n",
    "        placeAd = [x.replace('\\n','') for x in placeA]\n",
    "        placeAddress.append(placeAd[0])\n",
    "    except:\n",
    "        placeAddress.append('')\n",
    "\n",
    "    #finding the coordinates of the place        \n",
    "    try:\n",
    "        #this creates a list and divides the coordinates in Altitude & longitude\n",
    "        placeAlt1 = [x.get('data-coordinates') for x in soup.find_all('div',{'class':'DDPageSiderail__coordinates js-copy-coordinates'})]\n",
    "        newAlt = [coordinates.replace(',','') for string in placeAlt1 for coordinates in string.split()]\n",
    "    \n",
    "        placeAlt.append(float(newAlt[0]))\n",
    "        placeLong.append(float(newAlt[1]))\n",
    "    except:\n",
    "        placeAlt.append('')\n",
    "        placeLong.append('')\n",
    "    \n",
    "    #finding the editors for the page\n",
    "    try:\n",
    "        editors = [x.text for x in soup.find_all('a',{'class':'DDPContributorsList__contributor'})]\n",
    "        editors_clean = [x.replace('\\n','') for x in editors]\n",
    "        placeEditors.append(editors_clean)\n",
    "    except:\n",
    "        placeEditors.append('')\n",
    "    \n",
    "    #date of pubblication of the post\n",
    "    try:\n",
    "        Date = soup.find_all('div', {'class' : \"DDPContributor__name\"})[0].contents[0]\n",
    "        placePubDate1 = Date.replace(',','')\n",
    "        placePubDate.append(datetime.strptime(placePubDate1,'%B %d %Y'))\n",
    "    except:\n",
    "        placePubDate.append('')\n",
    "    \n",
    "    #Lists the place was included to\n",
    "    try:\n",
    "        placeRL1 = [x.text for x in soup.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "        placeRL = [x.replace('\\n', '') for x in placeRL1]\n",
    "        lun = len(placeRL)\n",
    "        placeRelatedLists.append(placeRL[lun-2:lun])\n",
    "    except:\n",
    "        placeRelatedLists.append('')\n",
    "        \n",
    "    #names of the related places\n",
    "    try:\n",
    "        placeRL1 = [x.text for x in soup.find_all('h3',{'class':'Card__heading --content-card-v2-title js-title-content'})]\n",
    "        placeRL = [x.replace('\\n', '') for x in placeRL1]\n",
    "        placeRelatedPlaces.append(placeRL[0:3])\n",
    "    except:\n",
    "        placeRelatedPlaces.append('')\n",
    "        \n",
    "    #the URL of the place\n",
    "    try:\n",
    "        url = [x.get('href') for x in soup.find_all('link', {'rel' : 'canonical'})]\n",
    "        placeURL.append(url[0])\n",
    "    except:\n",
    "        placeURL.append('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create d√¨the directory for the tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tsv_files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now need to write a function that creates the tsv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_files(i):\n",
    "    \n",
    "    #naming the columns in the tsv file\n",
    "    tsv_columns = ['placeName', 'placeTags', 'numPeopleVisited','numPeopleWant','placeDesc',\n",
    "                  'placeShortDesc','placeNearby','placeAddress','placeAlt','placeLong','placeEditors',\n",
    "                  'placePubDate','placeRelatedLists','placeRelatedPlaces','placeURL']\n",
    "    #getting the row for each tsv file\n",
    "    data = zip([placeName[cont-1]], [placeTags[cont-1]], [numPeopleVisited[cont-1]],[numPeopleWant[cont-1]],[placeDesc[cont-1]],\n",
    "                  [placeShortDesc[cont-1]],[placeNearby[cont-1]],[placeAddress[cont-1]],[placeAlt[cont-1]],[placeLong[cont-1]],[placeEditors[cont-1]],\n",
    "                  [placePubDate[cont-1]],[placeRelatedLists[cont-1]],[placeRelatedPlaces[cont-1]],[placeURL[cont-1]])\n",
    "    #creating and opening the tsv file\n",
    "    tsv_name = \"tsv_files/place_\" +str(i) + \".tsv\"\n",
    "    with open(tsv_name, encoding = 'utf-8', mode = 'w') as tsv_file:\n",
    "        tsv_output = csv.writer(tsv_file, delimiter='\\t')\n",
    "        tsv_output.writerow(tsv_columns)\n",
    "        for placeN, placeT, numP,numPW,placeD,placeSD,placeNear,placeAd,placeA,placeL,placeEd,placePubD,placeRL,placeRP,URL in data:\n",
    "            tsv_output.writerow([placeN, placeT, numP,numPW,placeD,placeSD,placeNear,placeAd,placeA,placeL,placeEd,placePubD,placeRL,placeRP,URL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_pages = 0\n",
    "cont_htmlcode = 0\n",
    "\n",
    "for pages in tqdm(range(0,400)):\n",
    "    for i in range(1,19):\n",
    "        path = 'C:/Users/flavi/Homeworks/ADM-HW3/HTML/' + 'page{}'.format(str(pages)) + '/_/' + 'htmlcode{}'.format(str(i)) + '.html'\n",
    "        f = open(path, encoding=\"utf8\")\n",
    "        extract_info(f)\n",
    "        # calling the tsv create function to create tsv \n",
    "        tsv_files(cont_htmlcode)\n",
    "        f.close()\n",
    "        cont_htmlcode += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create two different Search Engines that, given as input a query, return the places that match the query.\n",
    "\n",
    "First, you must pre-process all the information collected for each place by:\n",
    "\n",
    "1. Removing stopwords\n",
    "2. Removing punctuation\n",
    "3. Stemming\n",
    "4. Anything else you think it's needed\n",
    "\n",
    "For this purpose, you can use the nltk library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of the libraries that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of interest were saved in two empty lists: PlaceName in place_name and PlaceDesc in place_desc.\n",
    "\n",
    "Then, using the zip function, tuples were created between the two lists so that they could later be converted into a DataFrame (places_final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name = []\n",
    "place_desc = []\n",
    "\n",
    "for i in range(0, 7199):\n",
    "    places = open('TSV/place_' +str(i+1)+'.tsv','r',encoding=\"utf8\")\n",
    "    \n",
    "    place_info=pd.read_table(places)[['placeName','placeDesc']]\n",
    "\n",
    "    place_info.placeName = place_info.placeName.astype(str)\n",
    "    \n",
    "    place_info.placeDesc = place_info.placeDesc.astype(str)\n",
    "  \n",
    "    place_name.append(str(place_info.placeName[0]))\n",
    "\n",
    "    place_desc.append(str(place_info.placeDesc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists to DataFrame using the zip function\n",
    "places_tuples = list(zip(place_name,place_desc))\n",
    "\n",
    "places_final_df = pd.DataFrame(places_tuples, columns=['place_name','place_desc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the nltk library to get the english stopwords (stopwords). Also, PorterStemmer was used for stemming. The function stemming is in charge of extracting all the words from the sentences and then apply the porter stemmer function to perform the stemming correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwprds = stopwords.words('english')\n",
    "por_stem = PorterStemmer()\n",
    "\n",
    "def stemming(lines):\n",
    "\n",
    "    separated_word = lines.split()\n",
    "\n",
    "    stemmed_words = [por_stem.stem(word) for word in separated_word]\n",
    "\n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_final_df.place_desc = places_final_df.place_desc.apply(lambda x: ' '.join([word for word in x.split() if word not in (stpwprds)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/6j7jtq4n7m36jvtq0tfc4t880000gn/T/ipykernel_990/99722946.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  places_final_df.place_desc = places_final_df.place_desc.str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "#Regex was used to replace all characters that are not AlphaNumeric (\\w) or Whitspace characters (\\s)\n",
    "places_final_df.place_desc = places_final_df.place_desc.str.replace('[^\\w\\s]',' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the function stemming defined previously\n",
    "places_final_df.place_desc = places_final_df.place_desc.apply(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Conjunctive query\n",
    "\n",
    "For the first version of the search engine, we narrow our interest to the description of each place. It means that you will evaluate queries only concerning the place's description.\n",
    "\n",
    "Note: You should use the longer description placeDesc column and not the short description placeShortDesc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1) Create your index!\n",
    "Before building the index,\n",
    "\n",
    "Create a file named vocabulary, in the format you prefer, that maps each word to an integer (term_id).\n",
    "\n",
    "Then, the first brick of your homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "...}\n",
    "\n",
    "where document_i is the id of a document that contains that specific word.\n",
    "\n",
    "Hint: Since you do not want to compute the inverted index every time you use the Search Engine, it is worth thinking about storing it in a separate file and loading it in memory when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list of words (list_words) of each word that is going to use to create the vocabulary.\n",
    "\n",
    "set() was used to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words  = ' '.join([word for word in places_final_df.place_desc]).split()\n",
    "\n",
    "list_words_unique = set(list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dictionary \"vocabulary\" all the unique words were saved with an id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary = {}\n",
    "\n",
    "i=1\n",
    "for word in list_words_unique:\n",
    "    \n",
    "    vocabulary.update({i:word}) \n",
    "    \n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary was stored in a JSON file. Then, we opened this JSON file and extract the key and value of the data objects (vocab_ob). If the vocabulary value is in the place description (the sentences were separated by a split), a name was added to the list_inv. Finally, the initial dictionary (dictionary_inv) is updated with the key and the inverted list that was created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43478/43478 [1:05:49<00:00, 11.01it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"vocab.json\", \"w\") as file:\n",
    "    \n",
    "    json.dump(vocabulary, file)\n",
    "\n",
    "file.close()\n",
    "\n",
    "dictionary_inv = {}\n",
    "\n",
    "with open('vocab.json') as vocab_file:  \n",
    "    \n",
    "    vocab_ob = json.load(vocab_file)\n",
    "    \n",
    "    for key, value in tqdm(vocab_ob.items()):\n",
    "        \n",
    "        list_inv = []\n",
    "\n",
    "        for i in range(0,len(places_final_df)):\n",
    "            \n",
    "            if(value in places_final_df['place_desc'][i].split()): \n",
    "                place_name = 'place_'+str(i+1)\n",
    "                list_inv.append(place_name)\n",
    "                dictionary_inv.update({key:list_inv})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result (dictionary inv) was saved in the JSON inv_indx. In this way, we don't have to compute the inverted index every time we use the Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inv_indx.json\", \"w\") as file:\n",
    "    json.dump(dictionary_inv, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2) Execute the query\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "american museum\n",
    "\n",
    "The Search Engine is supposed to return a list of documents.\n",
    "\n",
    "What documents do we want?\n",
    "Since we are dealing with conjunctive queries (AND), each returned document should contain all the words in the query. The final output of the query must return, if present, the following information for each of the selected documents:\n",
    "\n",
    "placeName\n",
    "\n",
    "placeDesc\n",
    "\n",
    "placeURL\n",
    "\n",
    "If everything works well in this step, you can go to the next point and make your Search Engine more complex and better at answering queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function query_input_text makes a first loop to search if the query text is found in the vocabulary. If the word that the user is looking for is found in the vocabulary, the inv_indx file is loaded and if the vocabulary key (key_vocab) and the inv_indx key (key_inv) match, we proceed to store the inv values ‚Äã‚Äãin the initial list (places_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_input_text(place_info_search):\n",
    "    \n",
    "    places_list = []\n",
    "    \n",
    "    for word in place_info_search:\n",
    "        with open('vocab.json') as vocab_json:\n",
    "            vocabulary = json.load(vocab_json)\n",
    "            \n",
    "            for key_vocab, value_vocab in vocabulary.items():\n",
    "                \n",
    "                if(word == value_vocab):\n",
    "                    with open('inv_indx.json') as inv_indx:\n",
    "                        inverted_info = json.load(inv_indx)\n",
    "                        \n",
    "                        for key_inv, value_inv in inverted_info.items():\n",
    "                            \n",
    "                            if(key_vocab == key_inv):\n",
    "                                places_list.append(value_inv)\n",
    "    return places_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the first function \"places_list\" is a list that contains all the places that match any of the searched words.\n",
    "\n",
    "The function all_matches aims to return only the places that match ALL the words in the input search.\n",
    "\n",
    "First, a unique list is created containing all places that matched the search, then duplicates are removed by converting the list to a dictionary.\n",
    "\n",
    "Finally, a loop is made in which, for each place found, two things are compared: the length of the search words found in the vocabulary with the number of times that the place coincides with each search word found. If this comparison is equal, it means that every search word found in the vocabulary is in the place description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_matches(places_list):\n",
    "    places_list_all = []\n",
    "    final_places_all_query_matches =[]\n",
    "    \n",
    "    for word_places_general in range(len(places_list)):\n",
    "        for every_single_place in range(len(places_list[word_places_general])):\n",
    "            places_list_all .append(places_list[word_places_general][every_single_place])\n",
    "\n",
    "    place_unique = list(dict.fromkeys(places_list_all))\n",
    "\n",
    "    for place in place_unique:\n",
    "        if(len(places_list) == places_list_all.count(place)):\n",
    "            final_places_all_query_matches.append(place)\n",
    "\n",
    "    return final_places_all_query_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function result_places_info will return a DataFrame that contains all the information required by the Search Engine: Place Name, Place Description and URL.\n",
    "\n",
    "Through the for loop, the information of the places that mateched with ALL the previously searched words (places_unique_all_matches) was obtained.\n",
    "\n",
    "The info lists are converted to tuples using zip function and then these tuples are converted to DataFrame.\n",
    "\n",
    "As the example shown in the assignment had its information pre-processed, the description of the places is pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_places_info(places_unique_all_matches):\n",
    "    \n",
    "    place_name= []\n",
    "    place_desc = []\n",
    "    place_url = []\n",
    "    \n",
    "    for place in places_unique_all_matches:\n",
    "       \n",
    "        info_places_all_tsv = open('TSV/' +place+ '.tsv','r',encoding=\"utf8\")\n",
    "        \n",
    "        place_info = pd.read_table(info_places_all_tsv)[['placeName','placeDesc','placeURL']]\n",
    "        \n",
    "        place_info.placeName = place_info.placeName.astype(str)\n",
    "    \n",
    "        place_info.placeDesc = place_info.placeDesc.astype(str)\n",
    "\n",
    "        place_info.placeURL = place_info.placeURL.astype(str)\n",
    "  \n",
    "        place_name.append(str(place_info.placeName[0]))\n",
    "\n",
    "        place_desc.append(str(place_info.placeDesc[0]))\n",
    "\n",
    "        place_url.append(str(place_info.placeURL[0]))\n",
    "\n",
    "    place_url_final = [url[28:] for url in place_url]\n",
    "\n",
    "    #lists to DataFrame using the zip function\n",
    "    places_tuples = list(zip(place_name,place_desc,place_url_final))\n",
    "\n",
    "    places_final_df_first_se = pd.DataFrame(places_tuples, columns=['place_name','place_desc','place_url_final'])\n",
    "\n",
    "    #Pre-processing the final output\n",
    "\n",
    "    places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.apply(lambda x: ' '.join([word for word in x.split() if word not in (stpwprds)]))\n",
    "    places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.str.replace('[^\\w\\s]',' ')\n",
    "    places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.apply(stemming)\n",
    "    \n",
    "    return places_final_df_first_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search performed by the user. As all the information found in the description of the place is previously pre-processed, the same is done with the search text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/6j7jtq4n7m36jvtq0tfc4t880000gn/T/ipykernel_990/2176825905.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Search:')\n",
    "\n",
    "input_query = ' '.join([word for word in input_query.split() if word not in stpwprds])\n",
    "input_query = input_query.replace('[^\\w\\s]',' ')\n",
    "input_query = stemming(input_query)\n",
    "\n",
    "input_query_words = input_query.split()\n",
    "#First function to get the matches\n",
    "places_list = query_input_text(input_query_words)\n",
    "#Second function to get only places that have all lookup values\n",
    "places_unique_all_matches = all_matches(places_list)\n",
    "#Final DF result\n",
    "places_final_df_first_se = result_places_info(places_unique_all_matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_desc</th>\n",
       "      <th>place_url_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Witch House of Salem</td>\n",
       "      <td>the salem witchcraft trial took place februari...</td>\n",
       "      <td>/places/witch-house-salem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Museum of the Weird</td>\n",
       "      <td>the dime dime store museum account endang spec...</td>\n",
       "      <td>/places/museum-weird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>National Atomic Testing Museum</td>\n",
       "      <td>la vega oasi desert one sleep everi vice imagi...</td>\n",
       "      <td>/places/national-atomic-testing-museum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Canyons of the Ancients</td>\n",
       "      <td>ripe quiet reflect simpli awe inspir canyon an...</td>\n",
       "      <td>/places/canyons-of-the-ancients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Natural Bridge</td>\n",
       "      <td>often cite place among great wonder natur worl...</td>\n",
       "      <td>/places/the-natural-bridge-natural-bridge-virg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Bowling Ball Yard Art</td>\n",
       "      <td>updat octob 2020 some sculptur pictur longer v...</td>\n",
       "      <td>/places/bowling-ball-yard-art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Trees of Mystery</td>\n",
       "      <td>thi famili orient roadsid attract seen mile du...</td>\n",
       "      <td>/places/trees-of-mystery-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>De Vargas Street House</td>\n",
       "      <td>locat santa fe new mexico de varga street hous...</td>\n",
       "      <td>/places/de-vargas-street-house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Wilbur D. May Museum</td>\n",
       "      <td>there one place nevada find shrunken human hea...</td>\n",
       "      <td>/places/wilbur-d-may-museum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>The Liberty Bell Hiding Place</td>\n",
       "      <td>in septemb 1777 british defeat georg washingto...</td>\n",
       "      <td>/places/liberty-bell-hiding-place</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         place_name  \\\n",
       "0          The Witch House of Salem   \n",
       "1               Museum of the Weird   \n",
       "2    National Atomic Testing Museum   \n",
       "3           Canyons of the Ancients   \n",
       "4                The Natural Bridge   \n",
       "..                              ...   \n",
       "236           Bowling Ball Yard Art   \n",
       "237                Trees of Mystery   \n",
       "238          De Vargas Street House   \n",
       "239            Wilbur D. May Museum   \n",
       "240   The Liberty Bell Hiding Place   \n",
       "\n",
       "                                            place_desc  \\\n",
       "0    the salem witchcraft trial took place februari...   \n",
       "1    the dime dime store museum account endang spec...   \n",
       "2    la vega oasi desert one sleep everi vice imagi...   \n",
       "3    ripe quiet reflect simpli awe inspir canyon an...   \n",
       "4    often cite place among great wonder natur worl...   \n",
       "..                                                 ...   \n",
       "236  updat octob 2020 some sculptur pictur longer v...   \n",
       "237  thi famili orient roadsid attract seen mile du...   \n",
       "238  locat santa fe new mexico de varga street hous...   \n",
       "239  there one place nevada find shrunken human hea...   \n",
       "240  in septemb 1777 british defeat georg washingto...   \n",
       "\n",
       "                                       place_url_final  \n",
       "0                            /places/witch-house-salem  \n",
       "1                                 /places/museum-weird  \n",
       "2               /places/national-atomic-testing-museum  \n",
       "3                      /places/canyons-of-the-ancients  \n",
       "4    /places/the-natural-bridge-natural-bridge-virg...  \n",
       "..                                                 ...  \n",
       "236                      /places/bowling-ball-yard-art  \n",
       "237                         /places/trees-of-mystery-2  \n",
       "238                     /places/de-vargas-street-house  \n",
       "239                        /places/wilbur-d-may-museum  \n",
       "240                  /places/liberty-bell-hiding-place  \n",
       "\n",
       "[241 rows x 3 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_final_df_first_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Conjunctive query & Ranking score\n",
    "For the second search engine, given a query, we want to get the top-k (the choice of k it's up to you!) documents related to the query. In particular:\n",
    "\n",
    "Find all the documents that contain all the words in the query.\n",
    "\n",
    "Sort them by their similarity with the query.\n",
    "\n",
    "Return in output k documents, or all the documents with non-zero similarity with the query when the results are less than k. You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "To solve this task, you must use the tfIdf score and the Cosine similarity. The field to consider is still the placeDesc. Let's see how.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1) Inverted index\n",
    "\n",
    "Your second Inverted Index must be of this format:\n",
    "\n",
    "{\n",
    "term_id_1:[(document1, tfIdf_{term,document1}), (document2, tfIdf_{term,document2}), (document4, tfIdf_{term,document4}), ...],\n",
    "term_id_2:[(document1, tfIdf_{term,document1}), (document3, tfIdf_{term,document3}), (document5, tfIdf_{term,document5}), (document6, tfIdf_{term,document6}), ...],\n",
    "...}\n",
    "\n",
    "Practically, for each word, you want the list of documents in which it is contained and the relative tfIdf score.\n",
    "\n",
    "Tip: TfIdf values are invariant for the query. Due to this reason, you can precalculate and store them accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term_frequency returns the number of times a word (term) appears in a document (doc). Every documents has its own term frequency.\n",
    "\n",
    "Using the Regex Tokenizer we made a Counter about the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize_and_count(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return Counter(tokens)\n",
    "\n",
    "places_final_df[\"tokens_counter\"] = places_final_df.place_desc \\\n",
    "    .apply(tokenize_and_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens count\n",
    "places_final_df[\"tokens_count\"] = places_final_df \\\n",
    "    .tokens_counter \\\n",
    "    .apply(lambda counter: np.sum(list(counter.values()), dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_final_counts_df = pd.DataFrame({\n",
    "        'token' : places_final_df['tokens_counter'].apply(lambda counter: counter.keys()),\n",
    "        'count' : places_final_df['tokens_counter'].apply(lambda counter: counter.values())\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(term, place_id):\n",
    "    \n",
    "    count = places_final_df.tokens_counter[place_id - 1][term]\n",
    "    len_doc = places_final_df.tokens_count[place_id - 1]\n",
    "    \n",
    "    return count/len_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc_frequency returns the number of documents through a count (count) in which the term occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_places_final_counts_df = places_final_counts_df \\\n",
    "    .apply(pd.Series.explode) \\\n",
    "    .groupby(['token'])['count'].sum()\n",
    "\n",
    "doc_frequency_dict = grouped_places_final_counts_df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_frequency(term):\n",
    "\n",
    "    if term not in doc_frequency_dict:\n",
    "        print(f'term {term} is not in doc_frequency_dict, returning 0 as doc_frequency')\n",
    "        return 0\n",
    "    else:\n",
    "        return doc_frequency_dict[term]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the foor loop, \"term\" will save each word of the vocabulary and places related to each word (term) are saved in \"places\".\n",
    "\n",
    "The number of places (place_id) is obtained and with this the TF-IDF begins to be calculated. The math library was used to calculate the inverse of the document frequency. Finally, the tfidf is obtained by multiplying the term_frequency with the inverse of the document frequency and it is stored in the i_dictionary that is for each item (place) and finally in tfidf_dict the TF-IDF of each place is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.json\") as vocab_json:\n",
    "    vocabulary = json.load(vocab_json)\n",
    "    vocab_json.close()\n",
    "\n",
    "with open('inv_indx.json') as inv_indx:\n",
    "    inverted_info = json.load(inv_indx)\n",
    "    inv_indx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 29405/43478 [00:07<00:03, 3936.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term iÃábrahim is not in doc_frequency_dict, returning 0 as doc_frequency\n",
      "doc_frequency(iÃábrahim) == 0, taking 0 as inv_doc_freq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43478/43478 [00:11<00:00, 3925.42it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tfidf_dict = {}\n",
    "\n",
    "len_places_final_df = len(places_final_df)\n",
    "\n",
    "for i in tqdm(range(len(inverted_info))):\n",
    "\n",
    "    list_tfidf = []\n",
    "    \n",
    "    term = vocabulary[str(i+1)]\n",
    "    places = inverted_info[str(i+1)]\n",
    "\n",
    "    doc_f = doc_frequency(term)\n",
    "\n",
    "    if doc_f == 0:\n",
    "        \n",
    "        print(f'doc_frequency({term}) == 0, taking 0 as inv_doc_freq')\n",
    "        inv_doc_freq = 0\n",
    "   \n",
    "    else:\n",
    "        inv_doc_freq = math.log((len_places_final_df / doc_f + 1), 10)\n",
    "\n",
    "    for place in places:\n",
    "\n",
    "        place_id =(int(place.split(\"place_\",1)[1]))\n",
    "\n",
    "        term_f = term_frequency(term, place_id)\n",
    "\n",
    "        tfidf = inv_doc_freq * term_f\n",
    "\n",
    "        list_tfidf.append({place : tfidf})\n",
    "    \n",
    "    tfidf_dict.update({i+1:list_tfidf})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result (tfidf_dict) was saved in the JSON tfidf_final. In this way, we don't have to compute the inverted index every time we use the Search Engine #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_final.json\", \"w\") as file:\n",
    "    json.dump(tfidf_dict, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2) Execute the query\n",
    "In this new setting, given a query, you get the proper documents (i.e., those containing all the query's words) and sort them according to their similarity to the query. For this purpose, as the scoring function, we will use the Cosine Similarity concerning the tfIdf representations of the documents.\n",
    "\n",
    "Given a query input by the user, for example:\n",
    "\n",
    "american museum\n",
    "\n",
    "The search engine is supposed to return a list of documents, ranked by their Cosine Similarity to the query entered in the input.\n",
    "\n",
    "More precisely, the output must contain:\n",
    "\n",
    "1. placeName\n",
    "2. placeDesc\n",
    "3. placeURL\n",
    "4. The similarity score of the documents with respect to the query (float value between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine(v1,v2) is going to return the cosine between numerator calculation (num) and denominator calculation (den). \n",
    "\n",
    "inter is going to be the two vectors text intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(v1, v2):\n",
    "\n",
    "    inter = set(v1.keys() & set(v2.keys()))\n",
    "\n",
    "    num = sum([v1[x]*v2[x] for x in inter])\n",
    "\n",
    "    lv1 = list(v1.keys())\n",
    "    lv2 = list(v2.keys())\n",
    "\n",
    "    vector_1_sum = sum([v1[x] ** 2 for x in lv1])\n",
    "\n",
    "    vector_2_sum = sum([v2[x] ** 2 for x in lv2])\n",
    "\n",
    "    den = math.sqrt(vector_1_sum) * math.sqrt(vector_2_sum)\n",
    "\n",
    "    if not den:\n",
    "        return 0\n",
    "    else:\n",
    "        cos_ans = float(num)/float(den)\n",
    "        return cos_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vec_str is going to take a str value and convert it into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_str(text):\n",
    "\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "    ans_text = WORD.findall(text)\n",
    "    \n",
    "    return Counter(ans_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_query is the search carried out by the user, this search is pre-processed and all the places that match all the words in the search are obtained (places_final_df_first_se).\n",
    "\n",
    "Then, the place number (place_id) from places_final_df_first_se was obtained in a list (list_place_id).\n",
    "\n",
    "Finally, the result of the TF-IDF and the vocabulary were opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/6j7jtq4n7m36jvtq0tfc4t880000gn/T/ipykernel_990/2176825905.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Search:')\n",
    "\n",
    "input_query = ' '.join([word for word in input_query.split() if word not in stpwprds])\n",
    "input_query = input_query.replace('[^\\w\\s]',' ')\n",
    "input_query = stemming(input_query)\n",
    "\n",
    "input_query_words = input_query.split()\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "#First function to get the matches\n",
    "places_list = query_input_text(input_query_words)\n",
    "#Second function to get only places that have all lookup values\n",
    "places_unique_all_matches = all_matches(places_list)\n",
    "#Final DF result\n",
    "places_final_df_first_se = result_places_info(places_unique_all_matches)\n",
    "\n",
    "list_place_id = []\n",
    "\n",
    "for place in range(len(places_final_df)):\n",
    "    for place_fse in range(len(places_final_df_first_se)):\n",
    "\n",
    "        if(places_final_df_first_se.place_name[place_fse] == places_final_df.place_name[place]):\n",
    "            list_place_id.append(place+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.json\") as vocab_json:\n",
    "    vocabulary = json.load(vocab_json)\n",
    "    vocab_json.close()\n",
    "\n",
    "with open(\"tfidf_final.json\") as tfidf_final_json:\n",
    "    tfidf_ = json.load(tfidf_final_json)\n",
    "    tfidf_final_json.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function score_tfidf_ is going to return the  TF-IDF mean score (mean_score). Each word of the input search was obtained to perform the for loop. Each word (input_word) was searched for in the vocabulary and if it was found in the vocabulary, the match between the TF-IDF key and the key vocabulary was performed to obtain the score for each place. If the names matched, the place score was saved in the list: list_score_tfidf_.\n",
    "\n",
    "Finally, the average TF-IDF score (mean_score) was obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tfidf_(input_query, place_id_):\n",
    "    \n",
    "    input_query_words = input_query.split()\n",
    "    list_score_tfidf_ = []\n",
    "\n",
    "    for word in input_query_words:\n",
    "\n",
    "        for key_vocab, value_vocab in vocabulary.items():\n",
    "             if(word == value_vocab):\n",
    "                \n",
    "                for key_tfidf, value_tfidf in tfidf_.items():\n",
    "                    if(key_vocab ==key_tfidf):\n",
    "\n",
    "                        for i in range(len(tfidf_[key_tfidf])):\n",
    "                            \n",
    "                            for key_tfidf_, value_tfidf_ in tfidf_[key_tfidf][i].items():\n",
    "                                if(place_id_ == (int(key_tfidf_.split(\"place_\",1)[1]))):\n",
    "                                    list_score_tfidf_.append(value_tfidf_)\n",
    "    \n",
    "    mean_score = sum(list_score_tfidf_) / len(list_score_tfidf_) \n",
    "    return mean_score        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function similarity is going to return the similarity score (between 0 and 1) list_score_similarity. First, it's going to iterate over the final data frame that contains the places that match all the words searched by the user (places_final_df_first_se). Then, the place desc is obtained to calculate the vectors of both, the place description and the input query. Finally, the Cosine is performed, we obtained the Mean TF-IDF score for each place that matches the user search and we obtained the Similarity score.\n",
    "\n",
    "The Similarity column is attached to the DF of interest (places_final_df_first_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(list_place_id, input_query):\n",
    "\n",
    "    list_score_similarity = []\n",
    "    \n",
    "    for i in range(len(places_final_df_first_se)):\n",
    "        \n",
    "        place_desc_ = places_final_df['place_desc'][list_place_id[i]-1]\n",
    "       \n",
    "        v1 = vec_str(input_query)\n",
    "        v2 = vec_str(place_desc_)\n",
    "\n",
    "        #Cosine\n",
    "        cos_ans = cosine(v1, v2)\n",
    "        #Mean TF-IDF score\n",
    "        mean_score_ = score_tfidf_(input_query, list_place_id[i])\n",
    "        #Similarity\n",
    "        sim_score = mean_score_ *cos_ans\n",
    "        list_score_similarity.append(sim_score)\n",
    "   \n",
    "    return list_score_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_score_similarity = similarity(list_place_id, input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_final_df_first_se['similarity'] = list_score_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the data frame obtained is ordered taking into account the similarity by heapsort, as was required in the assignment, and we obtain the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_desc</th>\n",
       "      <th>place_url_final</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Lake Placid Murals</td>\n",
       "      <td>found 1992 lake placid mural societi fairli si...</td>\n",
       "      <td>/places/lake-placid-murals</td>\n",
       "      <td>0.011051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Museum of the Weird</td>\n",
       "      <td>the dime dime store museum account endang spec...</td>\n",
       "      <td>/places/museum-weird</td>\n",
       "      <td>0.009694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>The Throne of the Third Heaven of the Nations'...</td>\n",
       "      <td>behold result one man rent garag scrap fourtee...</td>\n",
       "      <td>/places/the-throne-of-the-third-heaven-of-the-...</td>\n",
       "      <td>0.009023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Harvard Museum of Natural History</td>\n",
       "      <td>collect three differ institut one massiv museu...</td>\n",
       "      <td>/places/harvard-museum-of-natural-history</td>\n",
       "      <td>0.008505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Senator</td>\n",
       "      <td>visit one oldest live organ world one point qu...</td>\n",
       "      <td>/places/the-senator-longwood-florida</td>\n",
       "      <td>0.007403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>American Banjo Museum</td>\n",
       "      <td>contain instrument public display collect worl...</td>\n",
       "      <td>/places/american-banjo-museum</td>\n",
       "      <td>0.006973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Drayton Hall</td>\n",
       "      <td>consid one beauti exampl georgian palladian ar...</td>\n",
       "      <td>/places/drayton-hall</td>\n",
       "      <td>0.006267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>U-505</td>\n",
       "      <td>one four remain u boat world u 505 fear fellow...</td>\n",
       "      <td>/places/u-505</td>\n",
       "      <td>0.006192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Laurinburg-Maxton Aircraft Boneyard</td>\n",
       "      <td>think aircraft boneyard might xa0pictur stretc...</td>\n",
       "      <td>/places/laurinburgmaxton-aircraft-boneyard</td>\n",
       "      <td>0.005920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Off the Rez Cafe</td>\n",
       "      <td>the u s govern s forc reloc nativ american com...</td>\n",
       "      <td>/places/off-the-rez-cafe</td>\n",
       "      <td>0.005801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            place_name  \\\n",
       "212                                 Lake Placid Murals   \n",
       "1                                  Museum of the Weird   \n",
       "225  The Throne of the Third Heaven of the Nations'...   \n",
       "27                   Harvard Museum of Natural History   \n",
       "147                                        The Senator   \n",
       "214                              American Banjo Museum   \n",
       "94                                        Drayton Hall   \n",
       "104                                              U-505   \n",
       "164                Laurinburg-Maxton Aircraft Boneyard   \n",
       "59                                    Off the Rez Cafe   \n",
       "\n",
       "                                            place_desc  \\\n",
       "212  found 1992 lake placid mural societi fairli si...   \n",
       "1    the dime dime store museum account endang spec...   \n",
       "225  behold result one man rent garag scrap fourtee...   \n",
       "27   collect three differ institut one massiv museu...   \n",
       "147  visit one oldest live organ world one point qu...   \n",
       "214  contain instrument public display collect worl...   \n",
       "94   consid one beauti exampl georgian palladian ar...   \n",
       "104  one four remain u boat world u 505 fear fellow...   \n",
       "164  think aircraft boneyard might xa0pictur stretc...   \n",
       "59   the u s govern s forc reloc nativ american com...   \n",
       "\n",
       "                                       place_url_final  similarity  \n",
       "212                         /places/lake-placid-murals    0.011051  \n",
       "1                                 /places/museum-weird    0.009694  \n",
       "225  /places/the-throne-of-the-third-heaven-of-the-...    0.009023  \n",
       "27           /places/harvard-museum-of-natural-history    0.008505  \n",
       "147               /places/the-senator-longwood-florida    0.007403  \n",
       "214                      /places/american-banjo-museum    0.006973  \n",
       "94                                /places/drayton-hall    0.006267  \n",
       "104                                      /places/u-505    0.006192  \n",
       "164         /places/laurinburgmaxton-aircraft-boneyard    0.005920  \n",
       "59                            /places/off-the-rez-cafe    0.005801  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_final_df_first_se = places_final_df_first_se.sort_values('similarity', ascending = False, kind ='heapsort').head(10)\n",
    "\n",
    "places_final_df_first_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a new score!\n",
    "\n",
    "Now it's your turn. Build a new metric to rank places based on the queries of their users.\n",
    "\n",
    "In this scenario, a single user can give input more information than a single textual query, so you need to consider all this information and think of a creative and logical way to answer the user's requests.\n",
    "\n",
    "Practically:\n",
    "\n",
    "    The user will enter a text query. As a starting point, get the query-related documents by exploiting the search engine of Step 3.1.\n",
    "\n",
    "    Once you have the documents, you need to sort them according to your new score. In this step, you won't have any more to take into account just the plot of the documents; you must use the remaining variables in your dataset (or new possible variables that you can create from the existing ones). You must use a heap data structure (you can use Python libraries) for maintaining the top-k documents.\n",
    "\n",
    "        Q: How to sort them? A: Allow the user to specify more information that you find in the documents and define a new metric that ranks the results based on the new request. You can also use other information regarding the place to score some places above others.\n",
    "\n",
    "N.B.: You have to define a scoring function, not a filter!\n",
    "\n",
    "The output, must contain:\n",
    "\n",
    "    placeName\n",
    "    placeDesc\n",
    "    placeURL\n",
    "    The new similarity score of the documents with respect to the query\n",
    "\n",
    "Are the results you obtain better than with the previous scoring function? Explain and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new score we decided is the following:\n",
    "\n",
    "\n",
    "$$ \\frac{similarity}{( \\frac{1}{numPeopleVisit}+\\frac{1}{numPeopleWant} )* 100}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of interest from the TSV files are loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def places_columns(list_place_id):\n",
    "    \n",
    "    place_numvisit= []\n",
    "    place_numwant = []\n",
    "    \n",
    "    for place in list_place_id:\n",
    "       \n",
    "        info_places = open('TSV/place_' +str(place)+ '.tsv','r',encoding=\"utf8\")\n",
    "\n",
    "        columns_places =pd.read_table(info_places)[['numPeopleVisited','numPeopleWant']]\n",
    "\n",
    "        columns_places.numPeopleVisited = columns_places.numPeopleVisited.astype(str)\n",
    "\n",
    "        columns_places.numPeopleWant = columns_places.numPeopleWant.astype(str)\n",
    "\n",
    "        place_numvisit.append(str(columns_places.numPeopleVisited[0]))\n",
    "\n",
    "        place_numwant.append(str(columns_places.numPeopleWant[0]))\n",
    "    \n",
    "    return place_numvisit, place_numwant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the number of people visited weight and number of people who want to go wight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_weight(place_numvisit, place_numwant):\n",
    "    \n",
    "    for place in range(len(place_numvisit)):\n",
    "        \n",
    "        place_numvisit[place] = 1/(float(place_numvisit[place]))\n",
    "        place_numwant[place] = 1/(float(place_numwant[place]))\n",
    "    \n",
    "    return place_numvisit, place_numwant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the New score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_score(result_similarity, weight_num_visit, weight_num_want):\n",
    "    \n",
    "    list_new_score = []\n",
    "\n",
    "    for place in range(len(result_similarity)):\n",
    "\n",
    "        new_score = result_similarity[place]/((weight_num_visit[place] + weight_num_want[place]) * 100)\n",
    "        list_new_score.append(new_score)\n",
    "\n",
    "    return list_new_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input search made by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/6j7jtq4n7m36jvtq0tfc4t880000gn/T/ipykernel_990/2176825905.py:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  places_final_df_first_se.place_desc = places_final_df_first_se.place_desc.str.replace('[^\\w\\s]',' ')\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Search:')\n",
    "\n",
    "input_query = ' '.join([word for word in input_query.split() if word not in stpwprds])\n",
    "input_query = input_query.replace('[^\\w\\s]',' ')\n",
    "input_query = stemming(input_query)\n",
    "\n",
    "input_query_words = input_query.split()\n",
    "#First function to get the matches\n",
    "places_list = query_input_text(input_query_words)\n",
    "#Second function to get only places that have all lookup values\n",
    "places_unique_all_matches = all_matches(places_list)\n",
    "#Final DF result\n",
    "places_final_df_first_se = result_places_info(places_unique_all_matches)\n",
    "\n",
    "#Obatining the place_id from the final result of the first search engine:\n",
    "list_place_id = []\n",
    "count=0\n",
    "\n",
    "for place in range(len(places_final_df)):\n",
    "    for place_fse in range(len(places_final_df_first_se)):\n",
    "\n",
    "        if(places_final_df_first_se.place_name[place_fse] == places_final_df.place_name[place]):\n",
    "\n",
    "            count= place+1\n",
    "            list_place_id.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obatining the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity Score\n",
    "result_similarity = similarity(list_place_id, input_query)\n",
    "\n",
    "#Number visited and Number Wanted columns\n",
    "place_numvisit, place_numwant = places_columns(list_place_id)\n",
    "\n",
    "#Weights\n",
    "weight_num_visit, weight_num_want = column_weight(place_numvisit, place_numwant)\n",
    "\n",
    "#New Score\n",
    "list_new_score = new_score(result_similarity,weight_num_visit, weight_num_want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame obtained is ordered taking into account the new_score by heapsort, as was required in the assignment, and we obtain the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_final_df_first_se['new_score'] = list_new_score\n",
    "\n",
    "places_final_df_first_se = places_final_df_first_se.sort_values(\"new_score\", ascending = False, kind =\"heapsort\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_name</th>\n",
       "      <th>place_desc</th>\n",
       "      <th>place_url_final</th>\n",
       "      <th>new_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Museum of the Weird</td>\n",
       "      <td>the dime dime store museum account endang spec...</td>\n",
       "      <td>/places/museum-weird</td>\n",
       "      <td>0.073182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Harvard Museum of Natural History</td>\n",
       "      <td>collect three differ institut one massiv museu...</td>\n",
       "      <td>/places/harvard-museum-of-natural-history</td>\n",
       "      <td>0.047643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Off the Rez Cafe</td>\n",
       "      <td>the u s govern s forc reloc nativ american com...</td>\n",
       "      <td>/places/off-the-rez-cafe</td>\n",
       "      <td>0.033783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Laurel Dinosaur Park</td>\n",
       "      <td>dure 18th 19th centuri clay format princ georg...</td>\n",
       "      <td>/places/laurel-dinosaur-park</td>\n",
       "      <td>0.031769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Frederick R. Weisman Art Museum</td>\n",
       "      <td>hous strike stainless steel brick build design...</td>\n",
       "      <td>/places/frederick-r-weisman-art-museum</td>\n",
       "      <td>0.027010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>V. C. Morris Gift Shop</td>\n",
       "      <td>thi build maiden lane downtown san francisco c...</td>\n",
       "      <td>/places/v-c-morris-gift-shop</td>\n",
       "      <td>0.023937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Crazy Horse Memorial</td>\n",
       "      <td>when carv mount rushmor began 1927 local lakot...</td>\n",
       "      <td>/places/crazy-horse-memorial</td>\n",
       "      <td>0.020849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>New Mexico Museum of Space History</td>\n",
       "      <td>most american familiar new mexico s place hist...</td>\n",
       "      <td>/places/new-mexico-museum-of-space-history</td>\n",
       "      <td>0.020044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Museum of Psychphonics</td>\n",
       "      <td>the museum psychphon modern day wunderkamm xa0...</td>\n",
       "      <td>/places/museum-of-psychphonics</td>\n",
       "      <td>0.019069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thorne Miniature Rooms</td>\n",
       "      <td>in depth museum art institut chicago carpet ro...</td>\n",
       "      <td>/places/thorne-miniature-rooms</td>\n",
       "      <td>0.017880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             place_name  \\\n",
       "1                   Museum of the Weird   \n",
       "27    Harvard Museum of Natural History   \n",
       "59                     Off the Rez Cafe   \n",
       "90                 Laurel Dinosaur Park   \n",
       "45      Frederick R. Weisman Art Museum   \n",
       "105              V. C. Morris Gift Shop   \n",
       "15                 Crazy Horse Memorial   \n",
       "133  New Mexico Museum of Space History   \n",
       "66               Museum of Psychphonics   \n",
       "31               Thorne Miniature Rooms   \n",
       "\n",
       "                                            place_desc  \\\n",
       "1    the dime dime store museum account endang spec...   \n",
       "27   collect three differ institut one massiv museu...   \n",
       "59   the u s govern s forc reloc nativ american com...   \n",
       "90   dure 18th 19th centuri clay format princ georg...   \n",
       "45   hous strike stainless steel brick build design...   \n",
       "105  thi build maiden lane downtown san francisco c...   \n",
       "15   when carv mount rushmor began 1927 local lakot...   \n",
       "133  most american familiar new mexico s place hist...   \n",
       "66   the museum psychphon modern day wunderkamm xa0...   \n",
       "31   in depth museum art institut chicago carpet ro...   \n",
       "\n",
       "                                place_url_final  new_score  \n",
       "1                          /places/museum-weird   0.073182  \n",
       "27    /places/harvard-museum-of-natural-history   0.047643  \n",
       "59                     /places/off-the-rez-cafe   0.033783  \n",
       "90                 /places/laurel-dinosaur-park   0.031769  \n",
       "45       /places/frederick-r-weisman-art-museum   0.027010  \n",
       "105                /places/v-c-morris-gift-shop   0.023937  \n",
       "15                 /places/crazy-horse-memorial   0.020849  \n",
       "133  /places/new-mexico-museum-of-space-history   0.020044  \n",
       "66               /places/museum-of-psychphonics   0.019069  \n",
       "31               /places/thorne-miniature-rooms   0.017880  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "places_final_df_first_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by the calculated New Score, the results obtained in the previous Search Engine that used only the Cosine Similarity are very similar to those obtained with the New Score. The difference in the Top 10 place is due to the number of people who have visited the place and the number of people who want to go there.\n",
    "\n",
    "So this new score is going to be ideal for people who want to use a Search Engine based on the popularity of a place, understand popularity by favoritism of the place¬†among¬†people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the most relevant places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This are the results of question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Imgs/Q3res.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locations with names and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv('Q4_locations.csv')\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start creating the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = folium.Map(location = [40,-97],zoom_start=4.9)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    curr=locations.loc[i]\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[curr['longitude'], curr['latitude']],\n",
    "        popup = curr['location_name']+'.\\n'+curr['city']+','+curr['country'],\n",
    "        tooltip = folium.Tooltip(permanent=True, text=curr['location_name']+'.\\n'+curr['city']+','+curr['country'])\n",
    "    ).add_to(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folium_deepnote_show(m): #function to show map\n",
    "    data = m.get_root().render()\n",
    "    data_fixed_height = data.replace('width: 100%;height: 100%', 'width: 100%').replace('height: 100.0%;', 'height: 609px;', 1)\n",
    "    display(HTML(data_fixed_height))    \n",
    "\n",
    "folium_deepnote_show(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Imgs/Question_4_final_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also put an image file of the produced map in the Imgs folder in any case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  Command line question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this question is visible in our main repository under the name:\n",
    "commandline.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Theoretical question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok avgs\n"
     ]
    }
   ],
   "source": [
    "f = open(\"ApplicationsInfo.txt\", 'r')\n",
    "\n",
    "l = f.readline().split()\n",
    "\n",
    "n_students = int(l[0])\n",
    "n_grades = int(l[1])\n",
    "\n",
    "\n",
    "\n",
    "#build dictionary where key : list of grades\n",
    "\n",
    "d={}\n",
    "\n",
    "for i in range(n_students):\n",
    "\n",
    "    line = f.readline().split()\n",
    "\n",
    "    if line==[]: break #means that we finished reading the textfile\n",
    "\n",
    "    name = line[0]+' '+line[1]\n",
    "    \n",
    "    d[name] = line[2:]\n",
    "\n",
    "#calculate grades average for each student\n",
    "\n",
    "for line in d:\n",
    "\n",
    "    avg = 0\n",
    "\n",
    "    \n",
    "    for i in range(n_grades):\n",
    "        \n",
    "        avg+=int(d[line][i])\n",
    "\n",
    "    avg=round(float(avg/n_grades),2)\n",
    "\n",
    "    d[line]=avg #update dictionary\n",
    "\n",
    "print('ok avgs')\n",
    "l_students = []\n",
    "l_grades = []\n",
    "\n",
    "\n",
    "#build list for student names and corresponf√¨ding grades average\n",
    "\n",
    "for key in d:\n",
    "\n",
    "    l_students.append(key)\n",
    "\n",
    "    l_grades.append(d[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm1: Bubble sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bubbleSort(grades, students):\n",
    "    n = len(grades)\n",
    "\n",
    "    swap = False\n",
    "    \n",
    "    #iterates through all array elements\n",
    "    for i in range(n-1):\n",
    "        \n",
    "        # Last i elements are already in place\n",
    "        for j in range(0, n-i-1):\n",
    " \n",
    "            #swap if the element found is greater than the next element\n",
    "            if grades[j] > grades[j + 1]:\n",
    "                swap = True\n",
    "                grades[j], grades[j + 1] = grades[j + 1], grades[j]\n",
    "                students[j], students[j + 1] = students[j + 1], students[j]\n",
    "         \n",
    "        if not swap: #the array is sorted\n",
    "\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2: Dynamic Insertion Sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudocode\n",
    "\n",
    "create a list new_list \n",
    "\n",
    "for each $element$ of input list:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for each $element$ of new_list:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; insert element in new_list where (new_list[i-1] $\\leq$ $element$ $\\leq$ new_list[i+1])\n",
    "\n",
    "return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to insert element in list at index\n",
    "def list_insert(l, index, elem): #costs T(7) and Worst Space Complexity is O(n+1)\n",
    "    \n",
    "    temp1 = l[:index]\n",
    "    temp2 = l[index:]\n",
    "    \n",
    "    out=[]\n",
    "    \n",
    "    if(temp1!=[]): out+=temp1\n",
    "    \n",
    "    out.append(elem)\n",
    "    \n",
    "    if(temp2!=[]): out+=temp2\n",
    "    \n",
    "    return out\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def dynamicInsertion(grades,students):\n",
    "\n",
    "    out_students=[]\n",
    "    out_grades=[]\n",
    "    \n",
    "    out_students.append(students[0])\n",
    "    out_grades.append(grades[0])\n",
    "\n",
    "    for j in range(1,len(grades)): #O(N)\n",
    "\n",
    "        for i in range(j): #O(log(N))\n",
    "            \n",
    "            if (i==0 and grades[j]<=out_grades[i]): #if the element is the smallest of the local output list\n",
    "                \n",
    "                \n",
    "                out_grades = list_insert(out_grades, i, grades[j])\n",
    "                out_students = list_insert(out_students,i,students[j])\n",
    "\n",
    "                break\n",
    "            \n",
    "            elif (i==j-1 and grades[j]>=out_grades[i]):\n",
    "                \n",
    "                out_grades = list_insert(out_grades, i, grades[j])\n",
    "                out_students = list_insert(out_students,i,students[j])\n",
    "                \n",
    "                break\n",
    "            \n",
    "            elif (grades[j]>=out_grades[i-1] and grades[j]<=out_grades[i]): #if the element is the greatest of the local output list\n",
    "\n",
    "                out_grades = list_insert(out_grades, i, grades[j])\n",
    "                out_students = list_insert(out_students,i,students[j])\n",
    "\n",
    "                break\n",
    "                \n",
    "    return out_grades, out_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3: Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSort(grades,students):\n",
    "    if len(grades) > 1:\n",
    " \n",
    "        #middle of array\n",
    "        mid = len(grades)//2\n",
    " \n",
    "        #split into 2 halves\n",
    "        left_g = grades[:mid] #diuviding grades array\n",
    "        left_s = students[:mid] #dividing students array\n",
    " \n",
    "        #split into 2 halves\n",
    "        right_g = grades[mid:]\n",
    "        right_s = students[mid:]\n",
    " \n",
    "        #sort first half\n",
    "        mergeSort(left_g, left_s)\n",
    " \n",
    "        #sort second half\n",
    "        mergeSort(right_g, right_s)\n",
    " \n",
    "        i = j = k = 0\n",
    " \n",
    "        #copy arrays to temp arrays left[] and right[]\n",
    "        while i < len(left_g) and j < len(right_g):\n",
    "            if left_g[i] <= right_g[j]:\n",
    "                grades[k] = left_g[i]\n",
    "                students[k] = left_s[i]\n",
    "                i += 1\n",
    "            else:\n",
    "                grades[k] = right_g[j]\n",
    "                students[k] = right_s[j]\n",
    "                j += 1\n",
    "            k += 1\n",
    " \n",
    "        #check if any element was left\n",
    "        while i < len(left_g):\n",
    "            grades[k] = left_g[i]\n",
    "            students[k] = left_s[i]\n",
    "            i += 1\n",
    "            k += 1\n",
    " \n",
    "        while j < len(right_g):\n",
    "            grades[k] = right_g[j]\n",
    "\n",
    "            print(students,right_s)\n",
    "            \n",
    "            students[k] = right_s[j]\n",
    "            j += 1\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best and Worst case complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Bubble Sort$: \n",
    "\n",
    "The Bubble Sort Algorithm has the fastest best case complexity of $O(N)$ as it has to perform N checks. This means the array is already sorted.\n",
    "for worst case we get that both the internal and external loop are going to iterate $N$ times each, for a total time complexity of $O(N^2)$.\n",
    "\n",
    "$Dynamic Insertion Sort$: \n",
    "\n",
    "For best case this algorithm also has to perform N checks, so it has a best case complexity of $O(N)$.\n",
    "For worst case, assuming that the internal loop has time complexity of $O(log(N))$ because it's number of iterations goes from 1 to n for each iteration of the external loop, which also takes $N$ iterations. Complexity is $O(N \\times log(N))$.\n",
    "\n",
    "$Merge Sort$: \n",
    "\n",
    "The Merge Sort has the same complexity for best, average and worst case, which is always $O(N  log(N))$.\n",
    "\n",
    "Keep in mind that worst case complexity is far more important than best case complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating the bar plot\n",
    "\n",
    "algorithms = ['Bubble Sort', 'Dynamic Insertion Sort', 'Merge Sort']\n",
    "times = [259.02, 151.49, 143.85]\n",
    "\n",
    "plt.bar(algorithms,times, color ='maroon',\n",
    "        width = 0.4)\n",
    " \n",
    "\n",
    "plt.ylabel(\"Running Time (seconds)\")\n",
    "plt.title(\"Running Times for N=50000\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most optimal algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most optimal algorithm is Algorithm 3: Merge Sort for two reasons.\n",
    "\n",
    "First, it has the lowest running time of 143.85 seconds and an asymptotycal time complexity of $O(N \\times log(N))$ like the Dynamic Insertion Sort, but it also has a lower space complexity because it does not need to create an array of length N while the Dynamic Insertion Sort needs $O(N)$ space to output the sorted list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "167eec726b819e0c6a70093e6eca930a95ad1db30271bf8923599164b221a31b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
